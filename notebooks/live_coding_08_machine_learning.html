
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>14. Machine Learning - Introduction &#8212; Data Science for (not yet) scientists</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/live_coding_08_machine_learning';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="15. Machine Learning - Common Algorithms" href="live_coding_09_machine_learning_algorithms.html" />
    <link rel="prev" title="13. Dimensionality Reduction" href="live_coding_07_dimensionality_reduction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../book/cover.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/data_science_cover_illustration_logo.png" class="logo__image only-light" alt="Data Science for (not yet) scientists - Home"/>
    <script>document.write(`<img src="../_static/data_science_cover_illustration_logo.png" class="logo__image only-dark" alt="Data Science for (not yet) scientists - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../book/cover.html">
                    Introduction to Data Science (for not-yet scientists)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/intro.html">1. Introduction: Data Science for Not-Yet-Scientists</a></li>
<li class="toctree-l1"><a class="reference internal" href="../book/01_intro_data_science.html">2. What is Data Science?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../book/02_data_science_ethics_society.html">3. Data Science, Ethics, and Society</a></li>
<li class="toctree-l1"><a class="reference internal" href="../book/03_use_of_this_book.html">4. How to use this book (… if you ask us)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Science Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/04_data_and_types.html">5. Data and Data Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../book/05_data_information_knowledge.html">6. Data - Information - Knowledge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../book/06_data_science_workflow.html">7. Data Science Workflow</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data acquisition and first exploration</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/07_data_acquisition_and_preparation.html">8. Data Acquisition &amp; Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="live_coding_04_distributions_statistical_measures.html">9. First Data Exploration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">In-depth Data Exploration</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="live_coding_05_correlation_analysis.html">10. Correlation Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="live_coding_06_clustering.html">11. Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="live_coding_06b_introduction_outlier_detection.html">12. Outlier Detection</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Modeling</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="live_coding_07_dimensionality_reduction.html">13. Dimensionality Reduction</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">14. Machine Learning - Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="live_coding_09_machine_learning_algorithms.html">15. Machine Learning - Common Algorithms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Working with text data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="live_coding_10_working_with_text_data.html">16. Introduction to Working with Text Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="live_coding_11_NLP_2_tokenization.html">17. NLP - basic techniques to analyse text data</a></li>
<li class="toctree-l1"><a class="reference internal" href="live_coding_11_NLP_3_tfifd_and_machine_learning.html">18. Computing with Text: Counting words</a></li>
<li class="toctree-l1"><a class="reference internal" href="live_coding_12_NLP_4_ngrams_word_vectors.html">19. Beyond Counting Individual Words: N-grams</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Look at the networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="live_coding_13_graphs.html">20. Networks / Graph Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="live_coding_14_graph_visualization.html">21. Visualizing Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="live_coding_14_graphs_part2.html">22. Bottlenecks, Hubs, Communities</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Next steps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="outlook.html">23. What are the next steps?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../book/bibliography.html">Bibliography</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Source Code and Contributions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/github.html">Source Code on GitHub</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/florian-huber/data_science_course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/florian-huber/data_science_course/issues/new?title=Issue%20on%20page%20%2Fnotebooks/live_coding_08_machine_learning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/live_coding_08_machine_learning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Machine Learning - Introduction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-machine-learning">14.1. What is Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-testing">14.2. Training &amp; Testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-know-how-good-our-model-is">14.2.1. How do we know how <em>good</em> our model is?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-split">14.2.2. Train/Test split</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-algorithms">14.3. Common algorithms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-k-nn">14.3.1. k-nearest neighbors (k-NN)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pros-cons-caveats">14.3.1.1. Pros, Cons, Caveats</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">14.4. </a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#want-to-read-more-on-this">14.4.1. Want to read more on this?</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="machine-learning-introduction">
<h1><span class="section-number">14. </span>Machine Learning - Introduction<a class="headerlink" href="#machine-learning-introduction" title="Link to this heading">#</a></h1>
<section id="what-is-machine-learning">
<h2><span class="section-number">14.1. </span>What is Machine Learning?<a class="headerlink" href="#what-is-machine-learning" title="Link to this heading">#</a></h2>
<p>There is still a lot of confusion about terms like <strong>artificial intelligence</strong> (or: <strong>A.I.</strong>), <strong>machine learning</strong>, and <strong>deep learning</strong>.
The aim of this course is not to give a full in-depth introduction to all those topics, because this clearly demands one (or usually: multiple) courses on its own.</p>
<p>However, what this section should show is, that the underlying idea of machine learning is often quite approachable. And the some of the respective techniques can be used with relative ease, at least from the code perspective. If you haven’t learnt the basics of machine learning before, it is unlikely that this section will make you feel like you have mastered the art. But, again, this is not the main goal.</p>
<p>Machine learning is a subfield of <em>artificial intelligence</em> (see <a class="reference internal" href="live_coding_07_dimensionality_reduction.html#fig-dimensionality-reduction01"><span class="std std-numref">Fig. 13.1</span></a>), which often makes this sound very intimidating at first. In its full complexity, this is indeed hard to master. But it is important to note at this point, that machine learning itself can also be seen as <em>just another tool</em> that we have as data scientists. In fact, many of its techniques are no more complicated or complex than methods in dimensionality reduction or clustering that we have seen in the last chapters (some even consider those as part of machine learning, but I will skip this discussion for now).</p>
<figure class="align-default" id="fig-ai-vs-ml-vs-dl">
<img alt="../_images/fig_ai_vs_ml_vs_deep_learning.png" src="../_images/fig_ai_vs_ml_vs_deep_learning.png" />
<figcaption>
<p><span class="caption-number">Fig. 14.1 </span><span class="caption-text">Machine Learning is a field of techniques that belongs to <em>artificial intelligence</em>. Currently, machine learning is even the main representation of artificial intelligence with <em>deep learning</em> being the most prominent subset. Here, however, we will focus on more classical machine learning techniques (no worries, those remain equally important in real-life practice!).</span><a class="headerlink" href="#fig-ai-vs-ml-vs-dl" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The key idea behind machine learning is that we use algorithms that <em>“learn from data”</em>. Learning here is nothing magic and typically means something very different from our everyday use of the word “learning” in a human context. Learning here simply means that the rules by which a program decides on its outcome or behavior are no longer <em>hard-coded</em> by a person, but they are automatically searched and optimized.</p>
<p>To give you an idea of how simple this “learning” can be: Imagine we have two highly correlated features, for instance the shoe size and the height of of many people. We could than automatically find a good linear fit function and then use this to make predictions for new data entries. Say, you find footprints of size 47 after a robery, then your new “machine learning model” (yes, that’s simply the linear fit!) can predict the height of that person (<a class="reference internal" href="#fig-predictions-already-done"><span class="std std-numref">Fig. 14.2</span></a><strong>A</strong>). The prediction is probably not perfect, but we have good reasons to believe that it most likely won’t be too far off either.</p>
<p>Or, think of several chat messages that we clustered into “spam”, “negative”, and “positive” and which is nicely reflected by their position in a 2D plot after dimensionality reduction (<a class="reference internal" href="#fig-predictions-already-done"><span class="std std-numref">Fig. 14.2</span></a><strong>B</strong>). If we now receive a new message which ends up clearly within the “positive” cluster, then we can obviously risk the first best guess of saying that this might be a positive message as well. Again, we could be wrong, but based on the data we have, this simply seems to be the best guess we can do.</p>
<p>So, when is this going to be called <em>machine learning</em>?
When we have an algorithm that does this guessing based on the data for us it is machine learning. If we do the guessing, it is not machine learning.</p>
<figure class="align-default" id="fig-predictions-already-done">
<img alt="../_images/fig_predictions_already_done.png" src="../_images/fig_predictions_already_done.png" />
<figcaption>
<p><span class="caption-number">Fig. 14.2 </span><span class="caption-text">Even before coming to this chapter, we have already worked with techniques that would actually allow to make predictions on the basis of known data points. <strong>A</strong>, when we looked at (high) correlations this already implied that there is a more or less reliable link between different features. <strong>B</strong>, when we think of clustering and dimensionality reduction it seems rather obvious that we could make predictions for new datapoints based on their lower dimensional position!</span><a class="headerlink" href="#fig-predictions-already-done" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Before we have a look at some actual machine learning techniques, we should look at one key distinction to make. In machine learning we distinguish two types of tasks that a model can do. <strong>Classification</strong> means that models will predict the class of unknown datapoints based on what they “learned” from <strong>labeled data</strong>. Labeled data means that this is data for which we <em>know</em> the true labels (also called: targets or ground truth).</p>
<p><strong>Regression</strong> means that models will predict one (or multiple) numerical values for unknown datapoints, see <a class="reference internal" href="#fig-classification-regression"><span class="std std-numref">Fig. 14.3</span></a>.</p>
<figure class="align-default" id="fig-classification-regression">
<img alt="../_images/fig_classification_regression.png" src="../_images/fig_classification_regression.png" />
<figcaption>
<p><span class="caption-number">Fig. 14.3 </span><span class="caption-text">In <em>machine learning</em> we distinguish <strong>classification</strong> and <strong>regression</strong> tasks. The key difference is that classification models will predict one out of several possible categories while regression models output numerical values (floats).</span><a class="headerlink" href="#fig-classification-regression" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="training-testing">
<h2><span class="section-number">14.2. </span>Training &amp; Testing<a class="headerlink" href="#training-testing" title="Link to this heading">#</a></h2>
<p>Before we start, let’s quickly repeat a few key terms:</p>
<ul class="simple">
<li><p><strong>model</strong>: you can think of a machine learning model as a program function. It takes a certain, well-defined input (the <strong>data</strong>, often lazyly written as <span class="math notranslate nohighlight">\(X\)</span>) and generates predictions or <strong>labels</strong> (lazyly written: <span class="math notranslate nohighlight">\(y\)</span>).</p></li>
<li><p>A model is <em>trained</em> on data with known labels. This is called <em>supervised learning</em>, because the process is guided by these target values (the labels). There is also something called <em>unsupervised learning</em>, but we will ignore this for now.</p></li>
<li><p>The prediction of labels is called <em>prediction</em> or <em>inference</em>. Obviously, this should happen after the model training.</p></li>
</ul>
<section id="how-do-we-know-how-good-our-model-is">
<h3><span class="section-number">14.2.1. </span>How do we know how <em>good</em> our model is?<a class="headerlink" href="#how-do-we-know-how-good-our-model-is" title="Link to this heading">#</a></h3>
<p>The tricky part about applying supervised machine learning models is that we <em>train</em> the model using (more or less-) well-understood data and later want to use its predictions on <em>unknown</em> data. This means on data where we either don’t know the labels ourselves (say, predicting tomorrow’s weather). Or, we could know the labels, but simply do not want or cannot manually create those for all cases (for instance classify all incoming mail as spam or no-spam). But can we trust the model’s predictions?</p>
<p>As the name says, everything the model outputs are <em>predictions</em>. For all models we will consider here, this means that every single model output is uncertain and could be wrong. With the tools we have at hand in this section, we cannot even estimate the chance of failing. But, we can measure how reliable our model is <em>on average</em>!</p>
<p>For this, we need to do the single most important action in all of machine learning, the <strong>train/test split</strong>.</p>
</section>
<section id="train-test-split">
<h3><span class="section-number">14.2.2. </span>Train/Test split<a class="headerlink" href="#train-test-split" title="Link to this heading">#</a></h3>
<p>For supervised machine learning we need data (<span class="math notranslate nohighlight">\(X\)</span>) and corresponding labels (<span class="math notranslate nohighlight">\(y\)</span>). To avoid blindly applying our model to truly unseen and unknown data, we virtually <strong>always</strong> split our data into training and test data. As the name suggests, the training data can be used to train our machine learning model. The test data is kept and will <strong>never</strong> be used in training, but only to assess the quality of predictions of our model. Typically, we want as much data as possible for the training since more data usually correlates with better models. However, we also have to reserve enough data for the test set to later guarantee a meaningful assessment of our model.</p>
<p>We will later see that this can easily be done by using the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function from <code class="docutils literal notranslate"><span class="pre">Scikit-Learn</span></code>.</p>
</section>
</section>
<section id="common-algorithms">
<h2><span class="section-number">14.3. </span>Common algorithms<a class="headerlink" href="#common-algorithms" title="Link to this heading">#</a></h2>
<p>Instead of going much deeper into the theory of what machine learning is, we will simply have a look at a few very common algorithms every well-trained data scientist should know about. Some of them remain standard items in our everyday toolbox and you will most likely apply them every now and then. Others are important because they reflect key concepts in machine learning algorithms and help you gain a more fundamental understanding of what is actually happening under the hood.</p>
<section id="k-nearest-neighbors-k-nn">
<h3><span class="section-number">14.3.1. </span>k-nearest neighbors (k-NN)<a class="headerlink" href="#k-nearest-neighbors-k-nn" title="Link to this heading">#</a></h3>
<p><strong><span class="math notranslate nohighlight">\(k\)</span>-nearest neighbors</strong> is for very good reasons one of the most commonly known machine learning algorithms. It is relatively intuitive and simple, yet still powerful enough to find plenty of use cases even today (despite havine much fancier techniques on the market).</p>
<p>The algorithm works as follows (<a class="reference internal" href="live_coding_09_machine_learning_algorithms.html#fig-knn-algorithm"><span class="std std-numref">Fig. 15.1</span></a>). For any given data point <span class="math notranslate nohighlight">\(x\)</span>, do the following:</p>
<ul class="simple">
<li><p>Search for the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors within the known data.</p></li>
<li><p>For classification: take the most common label of those <span class="math notranslate nohighlight">\(k\)</span> data points.</p></li>
<li><p>For regression: take the mean (or median) of those <span class="math notranslate nohighlight">\(k\)</span> data points.</p></li>
</ul>
<p>That’s essentially it, which means it is no more complicated than what was sketched in <a class="reference internal" href="#fig-predictions-already-done"><span class="std std-numref">Fig. 14.2</span></a>B. One could ask if the terms “training” or “learning” are very good in this context. But one could essentially just argue that the data provided for reference is available and hence “learned”. The algorithm is clearly a machine learning algorithm because we only define the process. The respective outcomes will be fully dependent on the provided reference data.</p>
<figure class="align-default" id="fig-knn-algorithm">
<img alt="../_images/fig_knn_algorithm.png" src="../_images/fig_knn_algorithm.png" />
<figcaption>
<p><span class="caption-number">Fig. 14.4 </span><span class="caption-text">k-nearest neighbors is a rather intuitive algorithm. It is fully distance based and relies on finding the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors within the reference data. Out of those data points the final prediction is generated, either by majority vote (classification) or by averaging (regression).</span><a class="headerlink" href="#fig-knn-algorithm" title="Link to this image">#</a></p>
</figcaption>
</figure>
<hr class="docutils" />
<section id="pros-cons-caveats">
<h4><span class="section-number">14.3.1.1. </span>Pros, Cons, Caveats<a class="headerlink" href="#pros-cons-caveats" title="Link to this heading">#</a></h4>
<p>Conceptually, the k-nearest neighbors algorithm is rather simple and intuitive. However, there are a few important aspects to consider when applying this algorithm.</p>
<p>First of all, k-nearest kneighbors is a distance-based algorithm. This means that we have to ensure that closer really means “more similar” which is not as simple as it maybe sounds. We have to decide on a <em>distance metric</em> that is the measure (or function) by which we calculate the distance between data points. We can use common metrics like the Euclidean distance, but there are many different options to choose from.
Even more critical is the proper <em>scaling</em> of our features. Just think of an example. We want to predict the shoe size of a person from the person’s height (measured in <span class="math notranslate nohighlight">\(m\)</span>) and weight (measured in <span class="math notranslate nohighlight">\(kg\)</span>). This means that we have two features here, height and weight. For a prediction on a new person we simply need his/her height and weight. Then k-NN will compare those values to all known (“learned”) data points in our model and find the closest <span class="math notranslate nohighlight">\(k\)</span> other people. If we now use the Euclidean distance, the distance <span class="math notranslate nohighlight">\(d\)</span> will simply be</p>
<p>$<span class="math notranslate nohighlight">\( d = \sqrt{(w_1 - w_2) ^ 2 + (h_1 - h_2) ^ 2} \)</span><span class="math notranslate nohighlight">\(, where \)</span>w<span class="math notranslate nohighlight">\( and \)</span>h$ are the weights and heights of person 1 and 2.</p>
<p>Try to answer the following question: What is the problem here?</p>
<p>…?</p>
<p>Ok. The issue here is, that the weights are in kilograms (<span class="math notranslate nohighlight">\(kg\)</span>), so we are talking about values like 50, 60, 80, 100. The height, however, is measured in meters (<span class="math notranslate nohighlight">\(m\)</span>) such that values are many times smaller. As a result, having two people differ one meter in height (which is a lot) will count no more than one kilogram difference (which is close to nothing). Clearly not what we intuitively mean by “nearest neighbors”!</p>
<p>The solution to this is a proper <strong>scaling</strong> of our data. Often, we will simply apply one of the following two scaling methods:</p>
<ol class="arabic simple">
<li><p>MinMax Scaling - this means we linearly rescale our data such that the lowest occuring value becomes 0 and the highest value becomes 1.</p></li>
<li><p>Standard Scaling - here we rescale our data such that the mean value will be 0 and the standard deviation will be 1.</p></li>
</ol>
<p>Both methods might give you values that look awkward at first. Standard scaling, for instance, gives both positive and negative values so that our height values in the example could be -1.04 or +0.27. But don’t worry, the scaling is really only meant to be used for the machine learning algorithm itself.</p>
<p>Once we scaled our data, and maybe also picked the right distance metric (or used a good default, which will do for a start), we are technically good to apply k-NN.</p>
<p>But there are still some questions we need to consider.</p>
<p>The obvious one is: What should we use as <span class="math notranslate nohighlight">\(k\)</span>?<br />
This is the model’s main parameter and we are free to choose any value we like. And there is no simple best choice that always work. In practice the choice of <span class="math notranslate nohighlight">\(k\)</span> will depend on the number of data points we have, but also the distribution of data and the number of classes or parameter ranges. We usually want to pick odd values here to avoid draws as much as possible (imagine two nearest neighbors are “spam” and two are “no-spam”). But whether 3, 5, 7, or 13 is the best choice will depend on our specific task at hand.</p>
<p>In machine learning we call such a thing a <strong>fitting parameter</strong>. This means that we are free to change its value and it might have a considerable impact on the quality of our predictions, or our “model performance”. Ideally we would compare several different models with different parameters and pick the one that performs best.</p>
<p>Let’s consider a situation as in <a class="reference internal" href="live_coding_09_machine_learning_algorithms.html#fig-knn-caveats"><span class="std std-numref">Fig. 15.2</span></a>A. Here we see that a change in <span class="math notranslate nohighlight">\(k\)</span> can lead to entirely different predictions for certain data points. In general, kNN predictions can be highly unstable close to border regions, and they also tend to be highly sensitive to the local density of data points. The later can be a problem if we have far more points of one category than for another.</p>
<figure class="align-default" id="fig-knn-caveats">
<img alt="../_images/fig_knn_caveats.png" src="../_images/fig_knn_caveats.png" />
<figcaption>
<p><span class="caption-number">Fig. 14.5 </span><span class="caption-text">k-nearest neighbors has a few important caveats. <strong>A</strong> its predictions can change with changing <span class="math notranslate nohighlight">\(k\)</span>, and generally are very density sensitive. <strong>B</strong> it suffers (as many machine learning models) from overconfidence, which simply means that it will confidently output predictions even for data points that are entirely different from the training data (or even physically impossible).</span><a class="headerlink" href="#fig-knn-caveats" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Finally, another common problem with kNN -but also many other models- is called <strong>over-confidence</strong> (<a class="reference internal" href="live_coding_09_machine_learning_algorithms.html#fig-knn-caveats"><span class="std std-numref">Fig. 15.2</span></a>B). The algorithm described here creates its predictions on the <span class="math notranslate nohighlight">\(k\)</span> closest neighbors. But for very unusual inputs or even entirely impossible inputs, the algorithm will still find <span class="math notranslate nohighlight">\(k\)</span> closest neighbors and make a prediction. So if you ask for the shoe size of a person of 6.20m and 840 kg your model might confidently answer your question and say: 48 (if nothing bigger occurred in the data). So much for the “intelligent” in <em>artificial intelligence</em> …</p>
<p>In summary, k-NN has a number of Pros and Cons:</p>
<p><strong>Pros</strong></p>
<ul class="simple">
<li><p>Can be used for classification and regression</p></li>
<li><p>Very intuitive, which also means that the predictions are easy to explain!</p></li>
<li><p>No overfitting (we will soon see what this is)</p></li>
<li><p>Does not make impossible predictions (because it only takes values from the training data)</p></li>
</ul>
<p><strong>Cons</strong></p>
<ul class="simple">
<li><p>Predictions are sensitive to local density of data points and the choice of <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p>Can suffer from over-confidence.</p></li>
<li><p>Does not scale well for very large datasets (computing all distances can take very long)</p></li>
</ul>
</section>
</section>
</section>
<section id="id1">
<h2><span class="section-number">14.4. </span><a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>k-nearest neighbors</p></li>
<li><p>linear regression</p></li>
<li><p>logistic regression</p></li>
<li><p>decision trees</p></li>
<li><p>random forests</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sb</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Material yet to be cleaned and prepared...</span>
</pre></div>
</div>
</div>
</div>
<section id="want-to-read-more-on-this">
<h3><span class="section-number">14.4.1. </span>Want to read more on this?<a class="headerlink" href="#want-to-read-more-on-this" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>https://github.com/GeostatsGuy/PythonNumericalDemos/tree/master</p></li>
<li><p>https://inferentialthinking.com/chapters/intro.html#</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="live_coding_07_dimensionality_reduction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Dimensionality Reduction</p>
      </div>
    </a>
    <a class="right-next"
       href="live_coding_09_machine_learning_algorithms.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">15. </span>Machine Learning - Common Algorithms</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-machine-learning">14.1. What is Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-testing">14.2. Training &amp; Testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-know-how-good-our-model-is">14.2.1. How do we know how <em>good</em> our model is?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-split">14.2.2. Train/Test split</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-algorithms">14.3. Common algorithms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-k-nn">14.3.1. k-nearest neighbors (k-NN)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pros-cons-caveats">14.3.1.1. Pros, Cons, Caveats</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">14.4. </a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#want-to-read-more-on-this">14.4.1. Want to read more on this?</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Florian Huber
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>