{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc45e4f",
   "metadata": {},
   "source": [
    "(ch_nlp_basics)=\n",
    "# NLP - Basic Techniques to Analyze Text Data\n",
    "\n",
    "Natural Language Processing (NLP) is an interdisciplinary field at the intersection of computer science, artificial intelligence, and linguistics. It focuses on enabling computers to process, interpret, and derive meaning from human language (see {numref}`fig_nlp_venn`). A central objective of NLP is to develop systems capable of \"reading\" and \"understanding\" text in order to perform tasks such as translation, summarization, sentiment analysis, and information extraction.\n",
    "\n",
    "```{figure} ../images/fig_nlp_venn.png\n",
    ":name: fig_nlp_venn\n",
    "\n",
    "Yes, again a Venn diagram. This time to illustrate that NLP is a highly interdisciplinary field with roots in computer science, AI, but also linguistics.\n",
    "```\n",
    "\n",
    "The importance of NLP in data science cannot be overstated. Every day, individuals and organizations generate massive volumes of text, ranging from social media updates and product reviews to customer support tickets and emails. NLP provides the tools and techniques necessary to make sense of this data in an automated or semi-automated fashion. By transforming unstructured text into structured data, NLP allows us to analyze and extract insights from human language, providing valuable context to support decision-making processes.\n",
    "\n",
    "Because NLP encompasses a broad set of methods (from rule-based pattern matching to modern neural language models), this chapter will introduce only the foundational techniques most commonly used in data science workflows. Our goal is to equip you with the basic tools needed to preprocess text, represent it numerically, and perform simple analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f84af2",
   "metadata": {},
   "source": [
    "## Example areas for the use of NLP techniques\n",
    "The following examples illustrate how broadly NLP can be applied and why it is so valuable:\n",
    "\n",
    "1. **Text Classification**  \n",
    "Text classification automatically assigns predefined categories or tags to text. By analyzing content and learning patterns, it streamlines tasks such as spam detection, news topic labeling, and document organization.\n",
    "\n",
    "3. **Sentiment Analysis**  \n",
    "Sentiment analysis uses NLP to detect and quantify the emotional tone of a text. Businesses rely on this technique to gauge customer opinions in reviews, social media posts, or survey responses, helping them make informed decisions based on positive, neutral, or negative sentiment.\n",
    "\n",
    "4. **Summarizationg**  \n",
    "Summarization distills lengthy documents into concise versions that preserve key points. It can be extractive (selecting important sentences) or abstractive (generating new summary sentences).\n",
    "\n",
    "6. **Spell Checking**  \n",
    "Spell-checking tools detect and suggest corrections for misspelled words. Modern approaches go beyond simple dictionary lookups by using context-aware models (e.g., probabilistic or neural methods) to correct typos or wrong word choices (e.g., \"their\" vs. \"there\"), which improves clarity and credibility in written text.\n",
    "\n",
    "8. **Machine Translation**  \n",
    "Machine translation (MT) converts text from one language to another. Contemporary MT systems‚Äîoften based on neural sequence-to-sequence architectures‚Äîhandle idiomatic expressions and context far better than older rule-based or phrase-based methods. MT accelerates multilingual communication, making large volumes of content accessible across languages.\n",
    "\n",
    "10. **Chatbots**  \n",
    "Chatbots leverage NLP to understand user queries, infer intent, and generate appropriate replies in real time. They combine components such as intent classification, entity recognition, dialogue management, and response generation. Modern chatbots are, for instance, used to handle customer service, information retrieval, and even casual conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae570b9",
   "metadata": {},
   "source": [
    "## Python NLP libraries\n",
    "\n",
    "There are several Python libraries that are popularly used for modern Natural Language Processing (NLP) tasks. Here are a few of the most commonly used ones:\n",
    "\n",
    "1. **NLTK (Natural Language Toolkit)**: This is a widely used library for symbolic and statistical NLP. It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet. NLTK also includes text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. It's excellent for teaching and working with the basics of NLP.\n",
    "2. **SpaCy**: This library is known for its advanced NLP capabilities and efficient performance. SpaCy is designed to handle large volumes of text, and its features include named entity recognition, part-of-speech tagging, dependency parsing, and sentence segmentation. Its flexibility and speed make it ideal for production-grade NLP tasks {cite}`vasiliev2020natural`.\n",
    "3. **Gensim**: This is a robust open-source vector space modeling and topic modeling toolkit. Gensim is designed to handle large text collections using data streaming and incremental algorithms, which is different from most other scientific software packages that only target batch and in-memory processing. It is especially good for tasks that involve topic modeling and document similarity analysis.\n",
    "4. **TextBlob**: This library simplifies text processing tasks by providing a consistent API for diving into common NLP tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, and more. TextBlob is very beginner-friendly and is an excellent choice for basic NLP tasks and for people getting started with NLP in Python.\n",
    "5. **Transformers (by Hugging Face)**: This library is based on the transformer architecture (like BERT, GPT, RoBERTa, XLM, etc) and has pre-trained models for many NLP tasks. It offers simple, yet powerful, APIs for performing tasks such as text classification, named entity recognition, translation, summarization, and more. It is a go-to library for state-of-the-art NLP (but clearly beyond the NLP basics that we will cover here).\n",
    "\n",
    "Here, we will work with **SpaCy** and **NLTK**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cabdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6874ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# NLP related libraries to work with text data\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392d73d-0d88-4ac8-a58a-bef15508606b",
   "metadata": {},
   "source": [
    "## NLP Preprocessing Workflow\n",
    "\n",
    "We usually work with text in various formats and sizes, for instance, from `.txt`, `.html`, or other structured or unstructured text file formats. For a later systematic data analysis or the training of machine-learning models, we first have to preprocess the text data consistently, typically done as sketched in {numref}`fig_nlp_processing_workflow`.\n",
    "\n",
    "```{figure} ../images/fig_nlp_processing_workflow.png\n",
    ":name: fig_nlp_processing_workflow\n",
    "\n",
    "Typically, an NLP preprocessing workflow consists of several stages, including raw text cleaning, tokenization, token cleaning, and token normalization. This is often the basis for later analysis or modeling steps.\n",
    "```\n",
    "\n",
    "\n",
    "### Raw Text Cleaning\n",
    "\n",
    "When you obtain or scrape text from various sources (web pages, PDFs, social media, logs), it often contains \"noise\" that can hinder analysis:\n",
    "\n",
    "- **HTML/XML tags and markup**: Words mixed with `<p>`, `<div>`, or other tags become fragmented tokens unless those tags are removed first.\n",
    "- **URLs, email addresses, and file paths**: Raw links (e.g., `https://‚Ä¶`) or email strings (`foo@example.com`) tend to inflate your vocabulary with nonsensical tokens.\n",
    "- **Irregular whitespace**: Multiple spaces, tabs, or stray line breaks can lead to empty or malformed tokens.\n",
    "- **Mixed casing and punctuation**: \"DataScience,\" \"Data Science,\" and \"data science\" may all appear separately unless you normalize case and handle punctuation uniformly.\n",
    "- **Contractions, emojis, and special characters**: In user‚Äêgenerated content (tweets, reviews), apostrophes (\"don‚Äôt\"), emoticons (\"üòä\"), or accented letters can either be informative or troublesome, depending on the task.\n",
    "\n",
    "The goal of raw‚Äêtext cleaning is to strip away or standardize these unwanted elements so that what remains is plain, continuous text composed of meaningful characters. In practice, cleaning might involve:\n",
    "\n",
    "1. Removing or replacing HTML/XML tags (so that `<strong>Bold</strong>` becomes simply \"Bold\").\n",
    "2. Stripping out URLs and email patterns (or substituting them with placeholder tokens like `<URL>` or `<EMAIL>` if you want to preserve their presence).\n",
    "3. Collapsing multiple spaces/newlines into single spaces.\n",
    "4. Converting all characters to lowercase (unless you plan to preserve casing for named‚Äêentity recognition or similar tasks).\n",
    "5. Handling punctuation‚Äîeither by removing most punctuation marks or by preserving certain symbols (like apostrophes) until the tokenization step.\n",
    "\n",
    "By cleaning first, you avoid feeding spurious tokens (e.g., `\"<a>\"`, `\"http://‚Ä¶\"`, or stray \"‚Äî\") into your tokenization process. Clean text provides a stable foundation for the next stage.\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Once the text is \"clean\" (i.e., markup removed, URLs taken out, whitespace normalized, and casing standardized), the next step is to split it into discrete units called **tokens**. Tokens typically represent words, but depending on your approach, they can also be punctuation marks, subwords, or even entire sentences.\n",
    "\n",
    "- **Word‚ÄêLevel Tokenization**\n",
    "   At its simplest, you break a cleaned string on spaces to get individual words. In practice, however, you often rely on more robust tokenizers that handle punctuation attached to words (e.g., `\"hello!\" ‚Üí [\"hello\", \"!\"]`) and contractions (`\"don‚Äôt\" ‚Üí [\"do\", \"n‚Äôt\"]`). The key point is that tokenization turns a long string into a list of word‚Äêlike elements you can process one at a time.\n",
    "- **Sentence‚ÄêLevel Tokenization (Optional)**\n",
    "   Some applications‚Äîsuch as summarization or sentiment analysis at the sentence level‚Äîrequire you to split a text into sentences before tokenizing words. In that case, you first identify sentence boundaries (e.g., by looking for periods, question marks, or line breaks) and then tokenize each sentence separately.\n",
    "- **Subword or Character‚ÄêLevel Tokenization (Beyond Basics)**\n",
    "   For many modern neural models, individual words are further split into subword units (e.g., \"running\" ‚Üí [\"run\", \"##ning\"]) or even individual characters. This helps handle rare or out‚Äêof‚Äêvocabulary words. We will cover those approaches in later chapters; for now, assume that tokens correspond roughly to English words or punctuation.\n",
    "\n",
    "After tokenization, you have a sequence such as:\n",
    "\n",
    "```\n",
    "[\"this\", \"is\", \"an\", \"example\", \"sentence\", \".\", \"it\", \"contains\", \"punctuation\", \"and\", \"mixed\", \"case\", \".\"]\n",
    "```\n",
    "\n",
    "Each of these tokens can now be examined, counted, or transformed in isolation.\n",
    "\n",
    "\n",
    "### Token Cleaning & Normalization\n",
    "\n",
    "Even after tokenization, many tokens still need further refinement before they become truly useful for feature extraction or modeling. Token cleaning and normalization typically include:\n",
    "\n",
    "1. **Lowercasing (if not already applied globally)**\n",
    "    Ensures that \"Data\" and \"data\" are treated identically rather than as two separate tokens.\n",
    "2. **Punctuation Stripping or Filtering**\n",
    "    If punctuation was not fully removed during raw‚Äêtext cleaning, you may drop tokens that consist only of punctuation (e.g., \".\", \",\", \"!\"). Alternatively, you can strip punctuation characters from the beginnings or ends of word tokens (e.g., `\"science,\" ‚Üí \"science\"`). Be careful: sometimes punctuation conveys meaning (e.g., \"U.S.A.\" or \"C++\"), so decide based on your specific use case.\n",
    "3. **Stop‚ÄêWord Removal (Optional)**\n",
    "    Common words like \"the,\" \"and,\" \"to,\" and \"is\" often appear so frequently that they contribute little to distinguishing meaning in tasks like topic classification. By removing these **stop words**, you reduce noise and shrink the vocabulary. However, for certain tasks (e.g., sentiment analysis or author attribution), you might retain pronouns and auxiliary verbs because they can carry important signals.\n",
    "4. **Digit and Special‚ÄêCharacter Handling (Optional)**\n",
    "    Some pipelines remove tokens that contain digits (e.g., \"2025\" or \"A1B2\"), replace them with a `<NUM>` placeholder, or leave them intact if numeric information is relevant (e.g., product codes, dates).\n",
    "5. **Contraction Expansion (If Deferred)**\n",
    "    If you did not expand contractions during raw‚Äêtext cleaning, you can handle them at this stage. For example, mapping \"don‚Äôt\" ‚Üí \"do not\" ensures that both \"do\" and \"not\" become separate, meaningful tokens rather than `\"don‚Äôt\"` which may not appear in your vocabulary.\n",
    "\n",
    "After token cleaning, your list of tokens might look like:\n",
    "\n",
    "```\n",
    "[\"this\", \"example\", \"sentence\", \"contains\", \"punctuation\", \"mixed\", \"case\"]\n",
    "```\n",
    "\n",
    "Notice that casing has been unified, purely punctuation tokens have been removed, and any stop words (e.g., \"is\", \"an\", \"and\") have been dropped. At this point, each token represents a distilled, meaningful unit ready for the next stages‚Äîsuch as converting to stems or lemmas, computing frequency counts, or feeding into a vectorization algorithm.\n",
    "\n",
    "------\n",
    "\n",
    "### How These Stages Fit Together\n",
    "\n",
    "1. **Raw Text Cleaning**\n",
    "    ‚Ä¢ Strips away extraneous markup, URLs, and inconsistent spacing.\n",
    "    ‚Ä¢ Produces one long, lowercase string of \"plain\" text.\n",
    "2. **Tokenization**\n",
    "    ‚Ä¢ Splits the cleaned string into atomic units (tokens).\n",
    "    ‚Ä¢ Creates a sequence (list) of tokens that still reflect some noise (e.g., punctuation, stop words).\n",
    "3. **Token Cleaning & Normalization**\n",
    "    ‚Ä¢ Further purifies tokens by removing punctuation‚Äêonly tokens, lowercasing (if needed), filtering out stop words, and handling numbers or contractions.\n",
    "    ‚Ä¢ Yields a final list of tokens that are consistent, comparable, and ready for stemming, lemmatization, or direct feature extraction.\n",
    "\n",
    "This entire process allows us to transform raw text data into a more digestible and analyzable format, preparing the ground for more advanced NLP techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b224f4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\flori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23a23a",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "Here, we demonstrate the NLTK processes of tokenization, stemming, and lemmatization.\n",
    "\n",
    "### Tokenization\n",
    "Tokenization is the process of splitting a large paragraph or text into sentences or words. These sentences or words are known as tokens. This is a crucial step in NLP as we often deal with words in text data.\n",
    "\n",
    "In this block of code, we import NLTK and define a string of text. The text contains a list of words with different grammatical forms. Using NLTK's `TreebankWordTokenizer`, we break down the text into individual words, or \"tokens\". The output is a list of these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189d1ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feet', 'cats', 'wolves', 'talking', 'talked', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"feet cats wolves talking talked?\"\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fd394",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is a process of reducing inflected (or sometimes derived) words to their word stem or root form‚Äîgenerally a written word form. The stem need not be identical to the morphological root of the word.\n",
    "\n",
    "Here, we create a `PorterStemmer` object and use it to find the root stem of each word in our list of tokens. The result is a list of these stems. You'll notice that the stems aren't always valid words (like 'wolv' for 'wolves'), as stemming operates on a rule-based approach without understanding the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcecb0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feet', 'cat', 'wolv', 'talk', 'talk', '?']\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "print([stemmer.stem(w) for w in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5222a",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization is the process of reducing inflected words to their word base or dictionary form. It's similar to stemming but is more accurate as it takes the context and meaning of the word into consideration.\n",
    "\n",
    "Instead of the `PorterStemmer`, we use NLTK's `WordNetLemmatizer` to find the dictionary base form (or lemma) of each word. This results in a list of lemmas. As you can see, lemmatization provides a more accurate root form ('wolf' for 'wolves') as compared to stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28994800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foot', 'cat', 'wolf', 'talking', 'talked', '?']\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "print([stemmer.lemmatize(w) for w in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03909d9",
   "metadata": {},
   "source": [
    "## NLP for languages other than English\n",
    "Natural Language Processing (NLP) is a truly global discipline, extending its reach to languages far beyond just English. \n",
    "\n",
    "However, it's worth noting that the effectiveness and ease of applying NLP techniques may vary across languages. For instance, languages with complex morphology like Finnish or Turkish, or those with little word delimitation like Chinese, can present unique challenges. Furthermore, resources and pre-trained models, especially those for machine learning, are more readily available for some languages, particularly English, than for others.\n",
    "\n",
    "### Let's try some German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d674b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuss', 'katz', 'wolf', 'sprech', 'gesproch', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"F√ºsse Katzen W√∂lfe sprechen gesprochen?\"  # Not an actual German sentence. Only some words for illustrative purposes.\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer(\"german\")\n",
    "print([stemmer.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca88326",
   "metadata": {},
   "source": [
    "## Applying SpaCy Models for Lemmatization\n",
    "\n",
    "**SpaCy** is a highly versatile and efficient Python library for Natural Language Processing (NLP). It offers comprehensive and advanced functionalities, outperforming NLTK in terms of efficiency and speed. You can find extensive details in [SpaCy's official documentation](https://spacy.io/usage/spacy-101).\n",
    "\n",
    "Having familiarized ourselves with the concept of lemmatization, let's now explore its practical application using SpaCy.\n",
    "\n",
    "Initially, you need to ensure that SpaCy and the relevant language models are installed in your environment. In the case of English, `en_core_web_sm` is a suitable model, whereas for German, `de_core_news_sm` can be utilized. SpaCy offers a variety of models for different languages which you can explore on the [SpaCy models page](https://spacy.io/usage/models/).\n",
    "\n",
    "\n",
    "Installation of SpaCy and downloading of language models can be performed via the following terminal commands:\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45932b5",
   "metadata": {},
   "source": [
    "Download the required language models first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9a18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if already installed\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    # If not, download the model\n",
    "    !python -m spacy download en_core_web_sm\n",
    "\n",
    "# There are many other models, here a small model for German:\n",
    "#!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dafb1a",
   "metadata": {},
   "source": [
    "Now that the models are installed, we can load the desired one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c568fe-9506-4e68-a6fe-914d4bf86f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24bd5c",
   "metadata": {},
   "source": [
    "Let's now define a text and pass it through the loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4acc55f-61af-4bfd-bc65-e36deeaa83c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feet cats wolves, speak, spoken?\n"
     ]
    }
   ],
   "source": [
    "text = \"Feet cats wolves, speak, spoken?\"\n",
    "doc = nlp(text)  # create NLP object\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5060157a-3f9c-45c8-9709-808d7e50d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, try an example in German:\n",
    "\n",
    "#nlp = spacy.load('de_core_news_lg')  # large german language model\n",
    "#nlp = spacy.load('de_core_news_sm')  # small german lanugage model \n",
    "\n",
    "#text = \"F√ºsse Katzen W√∂lfe sprechen gesprochen?\"\n",
    "#doc = nlp(text)  # create NLP object\n",
    "#print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c947744",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "By passing the text through the loaded NLP model, SpaCy already performs tokenization and a host of other operations under the hood: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ff5db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Feet', 'cats', 'wolves', ',', 'speak', ',', 'spoken', '?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d69a1",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Unlike **NLTK**, SpaCy has not option for **stemming**. But it provides many different language models (for many different languages) that allow for good **lemmatization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7102756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foot', 'cat', 'wolf', ',', 'speak', ',', 'speak', '?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed99490",
   "metadata": {},
   "source": [
    "Each word in the text is replaced with its base form or lemma, taking into account its usage in the sentence. This helps in text normalization, a critical step in text preprocessing for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1961e1",
   "metadata": {},
   "source": [
    "## Apply tokenization and lemmatization\n",
    "\"War of the worlds\" von H.G. Wells\n",
    "\n",
    "In the following, we will work with the text of the book \"War of the Worlds\" from H.G. Wells which is freely available via the [Gutenberg Project](https://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a4fe3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename and open the file\n",
    "filename = \"../datasets/wells_war_of_the_worlds.txt\"\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Perform some basic cleaning: replace newline characters with spaces\n",
    "text = text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56cdda8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338168"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many characters?\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6903e0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of The War of the Worlds, by H. G. Wells  This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.  Title: The War of the Worlds  Author: H. G. Wells  Release Date: July 1992 [eBook #36] [Most recently updated: November 27, 2021]  Language: English   *** START OF THE PROJECT GUTENBERG EBOOK THE WAR OF THE WORLDS ***  cover      The War of the Worlds  by H. G. Wells        ‚ÄòBut who shall dwell in these worlds if they be inhabited?     . . . Are we or they Lords of the World? . . . And     how are all things made for man?‚Äô                     KEPLER (quoted in _The Anatomy of Melan'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the first part of the text\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb34d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "# Create an NLP object by processing the text\n",
    "doc = nlp(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb07e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'War', 'of', 'the', 'Worlds', ',', 'by', 'H.', 'G.', 'Wells', ' ', 'This', 'eBook', 'is', 'for']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization: split the text into individual tokens (words)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens[:20]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2218ba",
   "metadata": {},
   "source": [
    "Now that we have all tokens of our book, we can obviously count the number of tokens (which is not the number of words!). But we can also look at how many different tokens there are by using the Python `set()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5566ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 71440\n",
      "Unique tokens: 7292\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of tokens and the number of unique tokens\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Unique tokens: {len(set(tokens))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ab883",
   "metadata": {},
   "source": [
    "Let us now do the same, but with **lemmatization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35e908ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'Project', 'Gutenberg', 'eBook', 'of', 'the', 'War', 'of', 'the', 'Worlds', ',', 'by', 'H.', 'G.', 'Wells', ' ', 'this', 'eBook', 'be', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'part', 'of', 'the', 'world', 'at', 'no', 'cost', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization: reduce each token to its base or root form\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cea1fb",
   "metadata": {},
   "source": [
    "We can also select tokens more specifically by using one of many attributes or methods from SpaCy (see [documentation](https://spacy.io/api/token/)). \n",
    "\n",
    "For instance: \n",
    "- `.is_punct` returns `True` if a token is a punctuation.\n",
    "- `.is_alpha` returns `True`if a token contains alphabetic characters\n",
    "- `.is_stop` returns `True` if word belongs to a so called \"stop list\" (less important words, we will come to this later)\n",
    "\n",
    "Since we here only want to count words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c121124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'Project', 'Gutenberg', 'eBook', 'of', 'the', 'War', 'of', 'the', 'Worlds', 'by', 'Wells', 'this', 'eBook', 'be', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'part', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restriction']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
    "print(lemmas[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ee8d147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lemmas: 60629\n",
      "Unique lemmas: 5589\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of lemmas and the number of unique lemmas\n",
    "print(f\"Total lemmas: {len(lemmas)}\")\n",
    "print(f\"Unique lemmas: {len(set(lemmas))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71603d5",
   "metadata": {},
   "source": [
    "By doing this, we are effectively shrinking the size of the dataset we are working with, while still retaining the essential meaning. It's worth noting that we also removed \"stop words\" - common words such as \"and\", \"the\", \"a\" - during lemmatization, which usually do not contain important information and are often removed in NLP.\n",
    "\n",
    "In the following steps, we could now investigate which words are the most common ones, we could identify named entities (such as people or places) or use this text data to train a machine learning model (like a text classifier or a sentiment analysis model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881028c",
   "metadata": {},
   "source": [
    "## Mini-Exercise!\n",
    "Why do we get more tokens than lemmas?\n",
    "Have a look at both and find the answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cfa25-23a7-43ea-8d36-97107586ab49",
   "metadata": {},
   "source": [
    "## Search specific word types or combinations\n",
    "With Spacy we can in principle also do more complex searches.\n",
    "We could, for instance search for nouns, verbs, or adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f16a5d8b-dd55-4042-b091-9d7fc9e2d63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[use, parts, world, cost, restrictions, terms, online, www.gutenberg.org, laws, country, Title, Language, START, worlds, things, man, I., FIGHTING, DESTRUCTION, WEYBRIDGE, CURATE, THUNDER, CHILD, FOOT, DEATH, CURATE, WORK, DAYS, COMING, I., one, years, century, world, intelligences, man, men, concerns, man, microscope]\n"
     ]
    }
   ],
   "source": [
    "nouns = [token for token in doc if token.pos_ == \"NOUN\"]\n",
    "print(nouns[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc5dcc-b684-4aa4-aba3-ee7d0badefd2",
   "metadata": {},
   "source": [
    "But we cannot only search all nouns or verbs, but also for specific combinations. As an example, we can search for all combinations of `like`or `love` with a noun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c3e266c-0e1f-4f69-8028-05fb57fb63ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like/love 4797 4799 like end\n",
      "like/love 6222 6224 like eyes\n",
      "like/love 8394 8396 like object\n",
      "like/love 8970 8972 like water\n",
      "like/love 9644 9646 like puffs\n",
      "like/love 12536 12538 like summer\n",
      "like/love 16160 16162 like daylight\n",
      "like/love 19145 19147 like parade\n",
      "like/love 21471 21473 like distance\n",
      "like/love 23134 23136 like thunderclaps\n",
      "like/love 29624 29626 like men\n",
      "like/love 32952 32954 like forms\n",
      "like/love 33577 33579 like ghosts\n",
      "like/love 37823 37825 like clerks\n",
      "like/love 42739 42741 like generator\n",
      "like/love 46982 46984 like mud\n",
      "like/love 49375 49377 like branches\n",
      "like/love 67580 67582 like sheep\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"LEMMA\": {\"IN\": [\"like\", \"love\"]}},\n",
    "            {\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"like/love\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c76bc3-7d76-4157-904e-dc4960612a64",
   "metadata": {},
   "source": [
    "Additionally, Spacy can be combined with regular expressions to create even more complex searches, but will not be shown here. Please consult Spacy's documentation in case you want to build more complex search patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29818729",
   "metadata": {},
   "source": [
    "## Chapter Summary and Outlook\n",
    "\n",
    "Throughout this chapter, we delved into the world of Natural Language Processing (NLP), exploring several key techniques for handling and processing text data effectively:\n",
    "\n",
    "- **Cleaning:** This is often the first step in processing text data, involving tasks like removing URLs, Emojis, and special characters, or replacing unwanted line breaks (`\"\\n\"`).\n",
    "- **Tokenization:** This involves breaking down text into smaller parts called tokens. Tokens can be as small as individual words or can even correspond to sentences or paragraphs, depending on the level of analysis required.\n",
    "- **Stemming:** Words can appear in different forms depending on gender, number, person, tense, and so on. Stemming involves reducing these words to their root or stem form. For example, the word \"finding\" could be stemmed to \"find\". This process is heuristic and sometimes may lead to non-meaningful stems.\n",
    "- **Lemmatization:** Similar to stemming, lemmatization aims to reduce words to their base form, but with a more sophisticated approach that takes vocabulary and morphological analysis into account. Lemmatization ensures that only the inflectional endings are removed, thus isolating the canonical form of a word known as a lemma. For example, \"found\" would be lemmatized to \"find\".\n",
    "- **Other Operations:** These could include removing numbers, punctuation marks, symbols, and stop words (commonly used words like \"and\", \"the\", \"a\", etc.), as well as converting text to lowercase for uniformity.\n",
    "\n",
    "We've also discussed the application of these concepts using powerful Python libraries like NLTK and SpaCy, which provide intuitive and efficient tools for dealing with NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ade0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
