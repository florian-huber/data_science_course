{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c627c46e",
   "metadata": {},
   "source": [
    "# Machine Learning - Introduction\n",
    "## What is Machine Learning?\n",
    "There is still a lot of confusion about terms like **artificial intelligence** (or: **A.I.**), **machine learning**, and **deep learning**.\n",
    "The aim of this course is not to give a full in-depth introduction to all those topics, because this clearly demands one (or usually: multiple) courses on its own.\n",
    "\n",
    "However, what this section should show is, that the underlying idea of machine learning is often quite approachable. And the some of the respective techniques can be used with relative ease, at least from the code perspective. If you haven't learnt the basics of machine learning before, it is unlikely that this section will make you feel like you have mastered the art. But, again, this is not the main goal.\n",
    "\n",
    "Machine learning is a subfield of *artificial intelligence* (see {numref}`fig_dimensionality_reduction01`), which often makes this sound very intimidating at first. In its full complexity, this is indeed hard to master. But it is important to note at this point, that machine learning itself can also be seen as *just another tool* that we have as data scientists. In fact, many of its techniques are no more complicated or complex than methods in dimensionality reduction or clustering that we have seen in the last chapters (some even consider those as part of machine learning, but I will skip this discussion for now)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e80df-b907-4d26-a856-acbff5933b4f",
   "metadata": {},
   "source": [
    "```{figure} ../images/fig_ai_vs_ml_vs_deep_learning.png\n",
    ":name: fig_ai_vs_ml_vs_dl\n",
    "\n",
    "Machine Learning is a field of techniques that belongs to *artificial intelligence*. Currently, machine learning is even the main representation of artificial intelligence with *deep learning* being the most prominent subset. Here, however, we will focus on more classical machine learning techniques (no worries, those remain equally important in real-life practice!).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32bef6-6667-47a5-9f68-8ea222f2b657",
   "metadata": {},
   "source": [
    "The key idea behind machine learning is that we use algorithms that *\"learn from data\"*. Learning here is nothing magic and typically means something very different from our everyday use of the word \"learning\" in a human context. Learning here simply means that the rules by which a program decides on its outcome or behavior are no longer *hard-coded* by a person, but they are automatically searched and optimized.\n",
    "\n",
    "To give you an idea of how simple this \"learning\" can be: Imagine we have two highly correlated features, for instance the shoe size and the height of of many people. We could than automatically find a good linear fit function and then use this to make predictions for new data entries. Say, you find footprints of size 47 after a robery, then your new \"machine learning model\" (yes, that's simply the linear fit!) can predict the height of that person ({numref}`fig_predictions_already_done`**A**). The prediction is probably not perfect, but we have good reasons to believe that it most likely won't be too far off either.\n",
    "\n",
    "Or, think of several chat messages that we clustered into \"spam\", \"negative\", and \"positive\" and which is nicely reflected by their position in a 2D plot after dimensionality reduction ({numref}`fig_predictions_already_done`**B**). If we now receive a new message which ends up clearly within the \"positive\" cluster, then we can obviously risk the first best guess of saying that this might be a positive message as well. Again, we could be wrong, but based on the data we have, this simply seems to be the best guess we can do. \n",
    "\n",
    "So, when is this going to be called *machine learning*?\n",
    "When we have an algorithm that does this guessing based on the data for us it is machine learning. If we do the guessing, it is not machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adcf06d-7ab1-49c2-96d8-5d7ead2099de",
   "metadata": {},
   "source": [
    "```{figure} ../images/fig_predictions_already_done.png\n",
    ":name: fig_predictions_already_done\n",
    "\n",
    "Even before coming to this chapter, we have already worked with techniques that would actually allow to make predictions on the basis of known data points. **A**, when we looked at (high) correlations this already implied that there is a more or less reliable link between different features. **B**, when we think of clustering and dimensionality reduction it seems rather obvious that we could make predictions for new datapoints based on their lower dimensional position!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3298670-ebd1-4a4b-8827-7725069806d3",
   "metadata": {},
   "source": [
    "Before we have a look at some actual machine learning techniques, we should look at one key distinction to make. In machine learning we distinguish two types of tasks that a model can do. **Classification** means that models will predict the class of unknown datapoints based on what they \"learned\" from **labeled data**. Labeled data means that this is data for which we *know* the true labels (also called: targets or ground truth).\n",
    "\n",
    "**Regression** means that models will predict one (or multiple) numerical values for unknown datapoints, see {numref}`fig_classification_regression`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05deeb96-c605-413b-bce1-f895ba788413",
   "metadata": {},
   "source": [
    "```{figure} ../images/fig_classification_regression.png\n",
    ":name: fig_classification_regression\n",
    "\n",
    "In *machine learning* we distinguish **classification** and **regression** tasks. The key difference is that classification models will predict one out of several possible categories while regression models output numerical values (floats).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9de29c-4100-4fae-ad0d-533077bd40d2",
   "metadata": {},
   "source": [
    "## Training & Testing\n",
    "\n",
    "Before we start, let's quickly repeat a few key terms:\n",
    "- **model**: you can think of a machine learning model as a program function. It takes a certain, well-defined input (the **data**, often lazyly written as $X$) and generates predictions or **labels** (lazyly written: $y$).\n",
    "- A model is *trained* on data with known labels. This is called *supervised learning*, because the process is guided by these target values (the labels). There is also something called *unsupervised learning*, but we will ignore this for now.\n",
    "- The prediction of labels is called *prediction* or *inference*. Obviously, this should happen after the model training.\n",
    "\n",
    "### How do we know how *good* our model is?\n",
    "The tricky part about applying supervised machine learning models is that we *train* the model using (more or less-) well-understood data and later want to use its predictions on *unknown* data. This means on data where we either don't know the labels ourselves (say, predicting tomorrow's weather). Or, we could know the labels, but simply do not want or cannot manually create those for all cases (for instance classify all incoming mail as spam or no-spam). But can we trust the model's predictions?\n",
    "\n",
    "As the name says, everything the model outputs are *predictions*. For all models we will consider here, this means that every single model output is uncertain and could be wrong. With the tools we have at hand in this section, we cannot even estimate the chance of failing. But, we can measure how reliable our model is *on average*!\n",
    "\n",
    "For this, we need to do the single most important action in all of machine learning, the **train/test split**.\n",
    "### Train/Test split\n",
    "For supervised machine learning we need data ($X$) and corresponding labels ($y$). To avoid blindly applying our model to truly unseen and unknown data, we virtually **always** split our data into training and test data. As the name suggests, the training data can be used to train our machine learning model. The test data is kept and will **never** be used in training, but only to assess the quality of predictions of our model. Typically, we want as much data as possible for the training since more data usually correlates with better models. However, we also have to reserve enough data for the test set to later guarantee a meaningful assessment of our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d132d6-ee47-41e3-ae58-3e2a94603318",
   "metadata": {},
   "source": [
    "We will later see that this can easily be done by using the `train_test_split` function from `Scikit-Learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557c698-f409-4430-abb9-b7da12b0a6d9",
   "metadata": {},
   "source": [
    "## Common algorithms\n",
    "\n",
    "Instead of going much deeper into the theory of what machine learning is, we will simply have a look at a few very common algorithms every well-trained data scientist should know about. Some of them remain standard items in our everyday toolbox and you will most likely apply them every now and then. Others are important because they reflect key concepts in machine learning algorithms and help you gain a more fundamental understanding of what is actually happening under the hood.\n",
    "\n",
    "\n",
    "### k-nearest neighbors (k-NN)\n",
    "**$k$-nearest neighbors** is for very good reasons one of the most commonly known machine learning algorithms. It is relatively intuitive and simple, yet still powerful enough to find plenty of use cases even today (despite havine much fancier techniques on the market).\n",
    "\n",
    "The algorithm works as follows ({numref}`fig_knn_algorithm`). For any given data point $x$, do the following:\n",
    "- Search for the $k$ nearest neighbors within the known data.\n",
    "- For classification: take the most common label of those $k$ data points.\n",
    "- For regression: take the mean (or median) of those $k$ data points.\n",
    "\n",
    "That's essentially it, which means it is no more complicated than what was sketched in {numref}`fig_predictions_already_done`B. One could ask if the terms \"training\" or \"learning\" are very good in this context. But one could essentially just argue that the data provided for reference is available and hence \"learned\". The algorithm is clearly a machine learning algorithm because we only define the process. The respective outcomes will be fully dependent on the provided reference data.\n",
    "\n",
    "```{figure} ../images/fig_knn_algorithm.png\n",
    ":name: fig_knn_algorithm\n",
    "\n",
    "k-nearest neighbors is a rather intuitive algorithm. It is fully distance based and relies on finding the $k$ nearest neighbors within the reference data. Out of those data points the final prediction is generated, either by majority vote (classification) or by averaging (regression).\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Pros, Cons, Caveats\n",
    "Conceptually, the k-nearest neighbors algorithm is rather simple and intuitive. However, there are a few important aspects to consider when applying this algorithm.\n",
    "\n",
    "First of all, k-nearest kneighbors is a distance-based algorithm. This means that we have to ensure that closer really means \"more similar\" which is not as simple as it maybe sounds. We have to decide on a *distance metric* that is the measure (or function) by which we calculate the distance between data points. We can use common metrics like the Euclidean distance, but there are many different options to choose from.\n",
    "Even more critical is the proper *scaling* of our features. Just think of an example. We want to predict the shoe size of a person from the person's height (measured in $m$) and weight (measured in $kg$). This means that we have two features here, height and weight. For a prediction on a new person we simply need his/her height and weight. Then k-NN will compare those values to all known (\"learned\") data points in our model and find the closest $k$ other people. If we now use the Euclidean distance, the distance $d$ will simply be\n",
    "\n",
    "$$ d = \\sqrt{(w_1 - w_2) ^ 2 + (h_1 - h_2) ^ 2} $$, where $w$ and $h$ are the weights and heights of person 1 and 2.\n",
    "\n",
    "Try to answer the following question: What is the problem here?\n",
    "\n",
    "...?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f96568-b274-4b9c-907d-26c9a057688c",
   "metadata": {},
   "source": [
    "Ok. The issue here is, that the weights are in kilograms ($kg$), so we are talking about values like 50, 60, 80, 100. The height, however, is measured in meters ($m$) such that values are many times smaller. As a result, having two people differ one meter in height (which is a lot) will count no more than one kilogram difference (which is close to nothing). Clearly not what we intuitively mean by \"nearest neighbors\"!\n",
    "\n",
    "The solution to this is a proper **scaling** of our data. Often, we will simply apply one of the following two scaling methods:\n",
    "1. MinMax Scaling - this means we linearly rescale our data such that the lowest occuring value becomes 0 and the highest value becomes 1.\n",
    "2. Standard Scaling - here we rescale our data such that the mean value will be 0 and the standard deviation will be 1.\n",
    "\n",
    "Both methods might give you values that look awkward at first. Standard scaling, for instance, gives both positive and negative values so that our height values in the example could be -1.04 or +0.27. But don't worry, the scaling is really only meant to be used for the machine learning algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378d369-08ee-4818-8f52-3213aa64f31f",
   "metadata": {},
   "source": [
    "Once we scaled our data, and maybe also picked the right distance metric (or used a good default, which will do for a start), we are technically good to apply k-NN.\n",
    "\n",
    "But there are still some questions we need to consider.\n",
    "\n",
    "The obvious one is: What should we use as $k$?  \n",
    "This is the model's main parameter and we are free to choose any value we like. And there is no simple best choice that always work. In practice the choice of $k$ will depend on the number of data points we have, but also the distribution of data and the number of classes or parameter ranges. We usually want to pick odd values here to avoid draws as much as possible (imagine two nearest neighbors are \"spam\" and two are \"no-spam\"). But whether 3, 5, 7, or 13 is the best choice will depend on our specific task at hand. \n",
    "\n",
    "\n",
    "In machine learning we call such a thing a **fitting parameter**. This means that we are free to change its value and it might have a considerable impact on the quality of our predictions, or our \"model performance\". Ideally we would compare several different models with different parameters and pick the one that performs best.\n",
    "\n",
    "Let's consider a situation as in {numref}`fig_knn_caveats`A. Here we see that a change in $k$ can lead to entirely different predictions for certain data points. In general, kNN predictions can be highly unstable close to border regions, and they also tend to be highly sensitive to the local density of data points. The later can be a problem if we have far more points of one category than for another.\n",
    "\n",
    "```{figure} ../images/fig_knn_caveats.png\n",
    ":name: fig_knn_caveats\n",
    "\n",
    "k-nearest neighbors has a few important caveats. **A** its predictions can change with changing $k$, and generally are very density sensitive. **B** it suffers (as many machine learning models) from overconfidence, which simply means that it will confidently output predictions even for data points that are entirely different from the training data (or even physically impossible).\n",
    "```\n",
    "\n",
    "Finally, another common problem with kNN -but also many other models- is called **over-confidence** ({numref}`fig_knn_caveats`B). The algorithm described here creates its predictions on the $k$ closest neighbors. But for very unusual inputs or even entirely impossible inputs, the algorithm will still find $k$ closest neighbors and make a prediction. So if you ask for the shoe size of a person of 6.20m and 840 kg your model might confidently answer your question and say: 48 (if nothing bigger occurred in the data). So much for the \"intelligent\" in *artificial intelligence* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb77256-aecd-43c5-a6a3-fdedeec3c408",
   "metadata": {},
   "source": [
    "In summary, k-NN has a number of Pros and Cons:\n",
    "\n",
    "**Pros**  \n",
    "- Can be used for classification and regression\n",
    "- Very intuitive, which also means that the predictions are easy to explain!\n",
    "- No overfitting (we will soon see what this is)\n",
    "- Does not make impossible predictions (because it only takes values from the training data)\n",
    "\n",
    "**Cons**  \n",
    "- Predictions are sensitive to local density of data points and the choice of $k$\n",
    "- Can suffer from over-confidence.\n",
    "- Does not scale well for very large datasets (computing all distances can take very long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de9430-74a2-49d8-a932-3dca6d6c883e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eddab27-a977-4232-bb19-fb3a7cec2f54",
   "metadata": {},
   "source": [
    "- k-nearest neighbors\n",
    "- linear regression\n",
    "- logistic regression\n",
    "- decision trees\n",
    "- random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a77333-5216-4766-996d-9723af411f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be518d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Material yet to be cleaned and prepared..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d854fb",
   "metadata": {},
   "source": [
    "### Want to read more on this?\n",
    "\n",
    "- https://github.com/GeostatsGuy/PythonNumericalDemos/tree/master\n",
    "- https://inferentialthinking.com/chapters/intro.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8970837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
